{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.data import Field, BucketIterator, TabularDataset\n",
    "import pandas as pd\n",
    "import model as MT\n",
    "import string\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17283</td>\n",
       "      <td>WASHINGTON  —   Congressional Republicans have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17284</td>\n",
       "      <td>After the bullet shells get counted, the blood...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17285</td>\n",
       "      <td>When Walt Disney’s “Bambi” opened in 1942, cri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17286</td>\n",
       "      <td>Death may be the great equalizer, but it isn’t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17287</td>\n",
       "      <td>SEOUL, South Korea  —   North Korea’s leader, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                            content\n",
       "0  17283  WASHINGTON  —   Congressional Republicans have...\n",
       "1  17284  After the bullet shells get counted, the blood...\n",
       "2  17285  When Walt Disney’s “Bambi” opened in 1942, cri...\n",
       "3  17286  Death may be the great equalizer, but it isn’t...\n",
       "4  17287  SEOUL, South Korea  —   North Korea’s leader, ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data/traindf.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114056, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(tokenize=get_tokenizer(\"basic_english\"),\n",
    "                            init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_text = TabularDataset(\n",
    "    path='data/valdf.csv', format='csv',\n",
    "    fields=[('id', None),('content',TEXT)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-0c71f86393a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data_text' is not defined"
     ]
    }
   ],
   "source": [
    "data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(data_text, vectors=\"glove.6B.100d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "for example in data_text.examples:\n",
    "    data_list.extend(example.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from sequential data, the batchify() function arranges the dataset into columns, trimming off any tokens remaining after the data has been divided into batches of size batch_size. For instance, with the alphabet as the sequence (total length of 26) and a batch size of 4, we would divide the alphabet into 4 sequences of length 6:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(data, bsz):\n",
    "    data = TEXT.numericalize([data])\n",
    "    # Divide the dataset into bsz parts.\n",
    "    nbatch = data.size(0) // bsz\n",
    "    # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "    data = data.narrow(0, 0, nbatch * bsz)\n",
    "    # Evenly divide the data across the bsz batches.\n",
    "    data = data.view(bsz, -1).t().contiguous()\n",
    "    return data.to(device)\n",
    "\n",
    "batch_size = 64\n",
    "batched_data = batchify(data_list, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([270543, 64])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "bptt = 35\n",
    "def get_batch(source, i):\n",
    "    seq_len = min(bptt, len(source) - 1 - i)\n",
    "    data = source[i:i+seq_len]\n",
    "    target = source[i+1:i+1+seq_len].view(-1)\n",
    "    return data, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hillary', 'can', 'here', 'an', 'than', '.', 'tuna', ')', 'is', 'being']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data,target = get_batch(batched_data, 1)\n",
    "ou = [TEXT.vocab.itos[i] for i in data[0,:10]]\n",
    "ou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([14662, 64])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntokens = len(TEXT.vocab) # the size of vocabulary\n",
    "emsize = 100 # embedding dimension\n",
    "nhid = 200 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 2 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 2 # the number of heads in the multiheadattention models\n",
    "dropout = 0.2 # the dropout value\n",
    "model = MT.TransformerModel(ntokens, emsize, nhead, nhid, nlayers, TEXT.vocab, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "lr = 5.0 # learning rate\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.95)\n",
    "\n",
    "import time\n",
    "def train():\n",
    "    model.train() # Turn on the train mode\n",
    "    total_loss = 0.\n",
    "    start_time = time.time()\n",
    "    ntokens = len(TEXT.vocab)\n",
    "    for batch, i in enumerate(range(0, batched_data.size(0) - 1, bptt)):\n",
    "        data, targets = get_batch(batched_data, i)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.view(-1, ntokens), targets)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        log_interval = 25\n",
    "        if batch % log_interval == 0 and batch > 0:\n",
    "            cur_loss = total_loss / log_interval\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches | '\n",
    "                  'lr {:02.2f} | ms/batch {:5.2f} | '\n",
    "                  'loss {:5.2f} | ppl {:8.2f}'.format(\n",
    "                    epoch, batch, len(batched_data) // bptt, scheduler.get_lr()[0],\n",
    "                    elapsed * 1000 / log_interval,\n",
    "                    cur_loss, math.exp(cur_loss)))\n",
    "            total_loss = 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(eval_model, data_source):\n",
    "    eval_model.eval() # Turn on the evaluation mode\n",
    "    total_loss = 0.\n",
    "    ntokens = len(TEXT.vocab.stoi)\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, data_source.size(0) - 1, bptt):\n",
    "            data, targets = get_batch(data_source, i)\n",
    "            output = eval_model(data)\n",
    "            output_flat = output.view(-1, ntokens)\n",
    "            total_loss += len(data) * criterion(output_flat, targets).item()\n",
    "    return total_loss / (len(data_source) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |    25/  418 batches | lr 5.00 | ms/batch 1765.47 | loss  9.06 | ppl  8568.35\n",
      "| epoch   1 |    50/  418 batches | lr 5.00 | ms/batch 1669.00 | loss  8.46 | ppl  4708.56\n",
      "| epoch   1 |    75/  418 batches | lr 5.00 | ms/batch 1672.68 | loss  8.15 | ppl  3465.62\n",
      "| epoch   1 |   100/  418 batches | lr 5.00 | ms/batch 1650.22 | loss  8.07 | ppl  3195.46\n",
      "| epoch   1 |   125/  418 batches | lr 5.00 | ms/batch 1687.56 | loss  7.82 | ppl  2489.16\n",
      "| epoch   1 |   150/  418 batches | lr 5.00 | ms/batch 1687.81 | loss  7.60 | ppl  1997.17\n",
      "| epoch   1 |   175/  418 batches | lr 5.00 | ms/batch 1639.03 | loss  7.49 | ppl  1798.03\n",
      "| epoch   1 |   200/  418 batches | lr 5.00 | ms/batch 1631.71 | loss  7.45 | ppl  1726.74\n",
      "| epoch   1 |   225/  418 batches | lr 5.00 | ms/batch 1637.90 | loss  7.25 | ppl  1401.46\n",
      "| epoch   1 |   250/  418 batches | lr 5.00 | ms/batch 1641.62 | loss  7.19 | ppl  1329.42\n",
      "| epoch   1 |   275/  418 batches | lr 5.00 | ms/batch 1665.08 | loss  7.07 | ppl  1177.61\n",
      "| epoch   1 |   300/  418 batches | lr 5.00 | ms/batch 1636.08 | loss  6.98 | ppl  1071.37\n",
      "| epoch   1 |   325/  418 batches | lr 5.00 | ms/batch 1669.15 | loss  6.97 | ppl  1061.78\n",
      "| epoch   1 |   350/  418 batches | lr 5.00 | ms/batch 1647.97 | loss  6.93 | ppl  1017.98\n",
      "| epoch   1 |   375/  418 batches | lr 5.00 | ms/batch 1642.41 | loss  6.84 | ppl   938.91\n",
      "| epoch   1 |   400/  418 batches | lr 5.00 | ms/batch 1636.64 | loss  6.76 | ppl   858.65\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   2 |    25/  418 batches | lr 4.51 | ms/batch 1759.00 | loss  6.87 | ppl   963.95\n",
      "| epoch   2 |    50/  418 batches | lr 4.51 | ms/batch 1741.93 | loss  6.61 | ppl   744.62\n",
      "| epoch   2 |    75/  418 batches | lr 4.51 | ms/batch 1721.72 | loss  6.52 | ppl   676.23\n",
      "| epoch   2 |   100/  418 batches | lr 4.51 | ms/batch 1720.29 | loss  6.57 | ppl   714.27\n",
      "| epoch   2 |   125/  418 batches | lr 4.51 | ms/batch 1716.46 | loss  6.52 | ppl   680.19\n",
      "| epoch   2 |   150/  418 batches | lr 4.51 | ms/batch 1711.77 | loss  6.43 | ppl   621.70\n",
      "| epoch   2 |   175/  418 batches | lr 4.51 | ms/batch 1687.24 | loss  6.42 | ppl   612.18\n",
      "| epoch   2 |   200/  418 batches | lr 4.51 | ms/batch 1733.32 | loss  6.47 | ppl   642.64\n",
      "| epoch   2 |   225/  418 batches | lr 4.51 | ms/batch 1655.30 | loss  6.35 | ppl   574.09\n",
      "| epoch   2 |   250/  418 batches | lr 4.51 | ms/batch 1681.60 | loss  6.40 | ppl   600.00\n",
      "| epoch   2 |   275/  418 batches | lr 4.51 | ms/batch 1755.29 | loss  6.37 | ppl   583.31\n",
      "| epoch   2 |   300/  418 batches | lr 4.51 | ms/batch 1707.45 | loss  6.26 | ppl   520.62\n",
      "| epoch   2 |   325/  418 batches | lr 4.51 | ms/batch 1654.13 | loss  6.30 | ppl   543.29\n",
      "| epoch   2 |   350/  418 batches | lr 4.51 | ms/batch 1655.81 | loss  6.32 | ppl   553.38\n",
      "| epoch   2 |   375/  418 batches | lr 4.51 | ms/batch 1659.85 | loss  6.31 | ppl   549.49\n",
      "| epoch   2 |   400/  418 batches | lr 4.51 | ms/batch 1604.80 | loss  6.21 | ppl   497.98\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n",
      "| epoch   3 |    25/  418 batches | lr 4.29 | ms/batch 1670.53 | loss  6.39 | ppl   594.55\n",
      "| epoch   3 |    50/  418 batches | lr 4.29 | ms/batch 1612.15 | loss  6.14 | ppl   465.65\n",
      "| epoch   3 |    75/  418 batches | lr 4.29 | ms/batch 1636.18 | loss  6.11 | ppl   451.37\n",
      "| epoch   3 |   100/  418 batches | lr 4.29 | ms/batch 1641.80 | loss  6.17 | ppl   478.48\n",
      "| epoch   3 |   125/  418 batches | lr 4.29 | ms/batch 1635.83 | loss  6.10 | ppl   443.82\n",
      "| epoch   3 |   150/  418 batches | lr 4.29 | ms/batch 1666.67 | loss  6.03 | ppl   417.00\n",
      "| epoch   3 |   175/  418 batches | lr 4.29 | ms/batch 1677.90 | loss  6.06 | ppl   426.29\n",
      "| epoch   3 |   200/  418 batches | lr 4.29 | ms/batch 1734.05 | loss  6.11 | ppl   448.21\n",
      "| epoch   3 |   225/  418 batches | lr 4.29 | ms/batch 1646.77 | loss  6.00 | ppl   403.48\n",
      "| epoch   3 |   250/  418 batches | lr 4.29 | ms/batch 1692.83 | loss  6.06 | ppl   427.38\n",
      "| epoch   3 |   275/  418 batches | lr 4.29 | ms/batch 1646.42 | loss  6.03 | ppl   417.27\n",
      "| epoch   3 |   300/  418 batches | lr 4.29 | ms/batch 1657.55 | loss  5.95 | ppl   384.56\n",
      "| epoch   3 |   325/  418 batches | lr 4.29 | ms/batch 1675.93 | loss  6.04 | ppl   420.77\n",
      "| epoch   3 |   350/  418 batches | lr 4.29 | ms/batch 1648.25 | loss  6.01 | ppl   405.70\n",
      "| epoch   3 |   375/  418 batches | lr 4.29 | ms/batch 1712.70 | loss  6.01 | ppl   407.75\n",
      "| epoch   3 |   400/  418 batches | lr 4.29 | ms/batch 1653.98 | loss  5.95 | ppl   385.11\n",
      "-----------------------------------------------------------------------------------------\n",
      "-----------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "best_val_loss = float(\"inf\")\n",
    "epochs = 3 # The number of epochs\n",
    "best_model = None\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train()\n",
    "    if epoch % 3 == 0:\n",
    "        torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': criterion,\n",
    "                'n_layers': nlayers,\n",
    "                'n_hidden':nhid,\n",
    "                'vocab':TEXT.vocab\n",
    "                }, f'./model_{epoch}_transformer.net')\n",
    "    val_loss = evaluate(model, val_data)\n",
    "    print('-' * 89)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | valid loss {:5.2f} | '\n",
    "          'valid ppl {:8.2f}'.format(epoch, (time.time() - epoch_start_time),\n",
    "                                     val_loss, math.exp(val_loss)))\n",
    "    print('-' * 89)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model,input_st):\n",
    "    data = TEXT.numericalize([input_st.split(' ')])\n",
    "    data.to(device)\n",
    "    with torch.no_grad():\n",
    "        pred = model(data)\n",
    "        out = torch.topk(pred, 1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = TEXT.vocab.itos\n",
    "out = sample(model,'hey')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate(model, input_sent, length_of_par):\n",
    "    out_sent = []\n",
    "    model.eval()\n",
    "    for i in range(length_of_par):\n",
    "        out = sample(model,input_sent)\n",
    "        next_word = TEXT.vocab.itos[out.indices[-1]]\n",
    "        input_sent += ' ' + next_word\n",
    "\n",
    "    return input_sent\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generate' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-83f1657d44b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'I like cucumber'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'generate' is not defined"
     ]
    }
   ],
   "source": [
    "output = generate(model,'I like cucumber', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'United states and other countries were not in the united states and other countries and other countries were not going to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be the first time of the first time of the first time of the first time of the first time of the first time'"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
